static vector_width = 8;

struct PassManager {
    alloc: fn (i64) -> Buffer,
    alloc_dynamic: fn(i64, i64) -> Buffer,
    release: fn (Buffer) -> ()
}

/* Utility functoins
TODO: these should probably be namespaced to not interfere with anything else?
*/

fn @round_down(n: i32, d: i32) -> i32 {
  (n / d) * d
}

/* Range functions using vector code */

fn @vector_range(body: fn(i32) -> ()) = @|start: i32, stop: i32| {
    let n = stop - start;
    if vector_width > 0 {
        let n_vec = round_down(n, vector_width);
        for i in range_step(0, n_vec, vector_width) {
            vectorize(vector_width, |j| @body(i + j + start))
        }
        for i in range(n_vec, n) {
            //vectorize(1, |j| @body(i + j + start))
            @body(i + start)
        }
    } else {
        for i in range(start, stop) {
            @body(i)
        }
    }
};

fn @vector_unroll(body: fn(i32) -> ()) = @|start: i32, stop: i32| {
    let n = stop - start;
    if vector_width > 0 {
        let n_vec = round_down(n, vector_width);
        for i in unroll_step(0, n_vec, vector_width) {
            vectorize(vector_width, |j| @body(i + j + start))
        }
        for i in unroll(n_vec, n) {
            //vectorize(1, |j| @body(i + j + start))
            @body(i + start)
        }
    } else {
        for i in unroll(start, stop) {
            @body(i)
        }
    }
};

fn @parallel_loop(body: fn(i32) -> ()) = @|n: i32| {
    if true {
        let num_threads = 4;
        for i in parallel(num_threads, 0, n) {
            @body(i)
        }
    } else {
        for i in range(0, n) {
            @body(i)
        }
    }
};

fn @atomic_add (p: &mut f32, d : f32) {
    atomic[f32](11, p, d, 7, "");
}

struct Tensor[T] {
    buffer : Buffer,
    num_dims : i32,
    size_fn: fn(i32) -> i64,
    access_fn: fn(&[i64]) -> &mut T
}

struct Tensor1[T] {
    buffer : Buffer,
    size: [i64 * 1],
    access_fn: fn([i64 * 1]) -> &mut T
}

struct Tensor2[T] {
    buffer : Buffer,
    size: [i64 * 2],
    access_fn: fn([i64 * 2]) -> &mut T
}

struct Tensor3[T] {
    buffer : Buffer,
    size: [i64 * 3],
    access_fn: fn([i64 * 3]) -> &mut T
}

struct Tensor4[T] {
    buffer : Buffer,
    size: [i64 * 4],
    access_fn: fn([i64 * 4]) -> &mut T
}

/*#[import(cc = "C", name = "get_buffer")] fn get_buffer_internal(_min_size : i64) -> &mut [i8];
#[import(cc = "C", name = "release_buffer")] fn release_buffer_internal(_buffer : &mut [i8]) -> ();
#[import(cc = "C", name = "print_num_allocs")] fn print_num_allocs() -> ();

fn @get_buffer_managed(min_size: i64) = Buffer {
    data = get_buffer_internal(min_size),
    size = min_size,
    device = 0
};

fn @release_buffer_managed(buffer : Buffer) {
    release_buffer_internal(buffer.data)
}*/

#[import(cc = "C", name = "load_matrix_dynamic")] fn load_matrix_dynamic(_tensor: &mut[f32], _file_name: &[u8], _matrix_name: &[u8]) -> ();
#[import(cc = "plugin", name = "static_alloca")] fn static_alloca(_size : i64) -> (fn() -> &mut [i8]);

#[intern]
fn load_matrix_into(tensor: Tensor[f32], file_name: &[u8], matrix_name: &[u8]) -> (fn () -> ()) {
    fn load_matrix() {
        let data = bitcast[&mut[f32]](tensor.buffer.data);
        load_matrix_dynamic(data, file_name, matrix_name);
    }

    load_matrix
}

#[intern]
fn load_tensor1_into(tensor: Tensor1[f32], file_name: &[u8], matrix_name: &[u8]) -> (fn () -> ()) {
    fn load_matrix() {
        let data = bitcast[&mut[f32]](tensor.buffer.data);
        load_matrix_dynamic(data, file_name, matrix_name);
    }

    load_matrix
}

#[intern]
fn load_tensor2_into(tensor: Tensor2[f32], file_name: &[u8], matrix_name: &[u8]) -> (fn () -> ()) {
    fn load_matrix() {
        let data = bitcast[&mut[f32]](tensor.buffer.data);
        load_matrix_dynamic(data, file_name, matrix_name);
    }

    load_matrix
}

#[intern]
fn load_tensor3_into(tensor: Tensor3[f32], file_name: &[u8], matrix_name: &[u8]) -> (fn () -> ()) {
    fn load_matrix() {
        let data = bitcast[&mut[f32]](tensor.buffer.data);
        load_matrix_dynamic(data, file_name, matrix_name);
    }

    load_matrix
}

#[intern]
fn load_tensor4_into(tensor: Tensor4[f32], file_name: &[u8], matrix_name: &[u8]) -> (fn () -> ()) {
    fn load_matrix() {
        let data = bitcast[&mut[f32]](tensor.buffer.data);
        load_matrix_dynamic(data, file_name, matrix_name);
    }

    load_matrix
}

//#[import(cc = "plugin", name = "load_matrix_into")] fn load_matrix_into(_tensor: Tensor[f32], _file_name: &[u8], _matrix_name: &[u8]) -> (fn () -> ());
//#[import(cc = "plugin", name = "static_alloca", depends = load_matrix_into)] fn static_alloca(_size : i64) -> (fn() -> &mut [i8]);
#[import(cc = "plugin", name = "static_release")] fn static_release(&mut [i8]) -> (fn() -> ());

#[import (cc = "plugin", name = "build_dynamic_array")] fn dynamic_array [T] (_element: T, _size: i64) -> (fn() -> &mut [T]);

fn @get_buffer_managed(min_size: i64) = Buffer {
    data = static_alloca(min_size)(),
    size = min_size,
    device = 0
};

fn @get_buffer_dynamic(init: i64, num_elements: i64) = Buffer {
    data = bitcast[&mut[i8]](dynamic_array(init, num_elements)()),
    size = sizeof[i64]() * num_elements,
    device = 0
};

fn @release_buffer_managed(_buffer : Buffer) { }

fn @sub_tensor[T] (manager: PassManager, input : Tensor[T], lower_bounds : &[i64], upper_bounds : &[i64]) -> Tensor[T] {
    let new_size_fn = @|i: i32| { upper_bounds(i) - lower_bounds(i) };

    let new_access_fn = |n : &[i64]| {
        let new_address_buffer = manager.alloc(sizeof[i64]() * input.num_dims as i64);
        let new_address = bitcast[&mut[i64]](new_address_buffer.data);

        for i in unroll(0, input.num_dims) {
            new_address(i) = n(i) + lower_bounds(i);
        }

        input.access_fn(new_address)
    };

    Tensor[T] {
        buffer = input.buffer,
        num_dims = input.num_dims,
        size_fn = new_size_fn,
        access_fn = new_access_fn
    }
}

/* Allocate and Release tensors, with different memory layouts as needed */

enum AllocLayout {
    RCK,
    KCR
}

fn @alloc_tensor_layout[T] (manager: PassManager, num_dims : i32, size_fn : fn(i32) -> i64, layout : AllocLayout) -> Tensor[T] {
    let mut size = 1 as i64;
    for i in unroll (0, num_dims) {
        size *= size_fn(i);
    }

    let buffer = manager.alloc(sizeof[T]() * size);

    let access_fn_rck = |n : &[i64]| {
        let mut address = n(num_dims - 1);
        for i in unroll(0, num_dims - 1) {
            address = address * size_fn(num_dims - 2 - i) + n(num_dims - 2 - i);
        }
        &mut bitcast[&mut[T]](buffer.data)(address)
    };

    let access_fn_kcr = |n : &[i64]| {
        let mut address = n(0);
        for i in unroll(1, num_dims) {
            address = address * size_fn(i) + n(i);
        }
        &mut bitcast[&mut[T]](buffer.data)(address)
    };

    Tensor[T] {
        buffer = buffer,
        num_dims = num_dims,
        size_fn = size_fn,
        access_fn = match layout { 
            AllocLayout::KCR => access_fn_kcr,
            AllocLayout::RCK => access_fn_rck,
        }
    }
}

fn release_tensor[T] (manager: PassManager, mat : Tensor[T]) -> () {
    manager.release(mat.buffer);
}

fn @alloc_tensor[T] (manager: PassManager, num_dims : i32, size_fn : fn(i32) -> i64) = @alloc_tensor_layout[T](manager, num_dims, size_fn, AllocLayout::KCR);
/*fn @alloc_tensor[T] (manager: PassManager, num_dims : i32, size_fn : fn(i32) -> i64) -> Tensor[T] {
    select(num_dims > 1 && size_fn(1) == 12,
            @alloc_tensor_layout[T](manager, num_dims, size_fn, AllocLayout::RCK),
            @alloc_tensor_layout[T](manager, num_dims, size_fn, AllocLayout::KCR))
}*/

#[intern] fn @alloc_tensor_f32 (manager: PassManager, num_dims: i32, size_fn : fn(i32) -> i64) = @alloc_tensor[f32](manager, num_dims, size_fn);
//#[intern] fn release_tensor_f32 (manager: PassManager, mat : Tensor[f32]) = release_tensor[f32](manager, mat);

#[intern] fn @alloc_initializer_f32 (manager: PassManager, num_dims: i32, size_fn : fn(i32) -> i64) = @alloc_tensor_layout[f32](manager, num_dims, size_fn, AllocLayout::RCK);

fn @alloc_tensor_layout_1[T] (manager: PassManager, size : [i64 * 1], layout : AllocLayout) -> Tensor1[T] {
    let mut size_buffer = 1 as i64;
    for i in unroll (0, 1) {
        size_buffer *= size(i);
    }

    let buffer = manager.alloc(sizeof[T]() * size_buffer);

    let access_fn_rck = |n : [i64 * 1]| {
        let mut address = n(0);
        &mut bitcast[&mut[T]](buffer.data)(address)
    };

    let access_fn_kcr = |n : &[i64 * 1]| {
        let mut address = n(0);
        &mut bitcast[&mut[T]](buffer.data)(address)
    };

    Tensor1[T] {
        buffer = buffer,
        size = size,
        access_fn = match layout { 
            AllocLayout::KCR => access_fn_kcr,
            AllocLayout::RCK => access_fn_rck,
        }
    }
}

#[intern] fn @alloc_tensor_1_f32 (manager: PassManager, size : [i64 * 1]) = @alloc_tensor_layout_1[f32](manager, size, AllocLayout::KCR);
#[intern] fn @alloc_initializer_1_f32 (manager: PassManager, size : [i64 * 1]) = @alloc_tensor_layout_1[f32](manager, size, AllocLayout::RCK);

fn @alloc_tensor_layout_2[T] (manager: PassManager, size : [i64 * 2], layout : AllocLayout) -> Tensor2[T] {
    let mut size_buffer = 1 as i64;
    for i in unroll (0, 2) {
        size_buffer *= size(i);
    }

    let buffer = manager.alloc(sizeof[T]() * size_buffer);

    let access_fn_rck = |n : [i64 * 2]| {
        let mut address = n(2 - 1);
        for i in unroll(0, 2 - 1) {
            address = address * size(2 - 2 - i) + n(2 - 2 - i);
        }
        &mut bitcast[&mut[T]](buffer.data)(address)
    };

    let access_fn_kcr = |n : [i64 * 2]| {
        let mut address = n(0);
        for i in unroll(1, 2) {
            address = address * size(i) + n(i);
        }
        &mut bitcast[&mut[T]](buffer.data)(address)
    };

    Tensor2[T] {
        buffer = buffer,
        size = size,
        access_fn = match layout { 
            AllocLayout::KCR => access_fn_kcr,
            AllocLayout::RCK => access_fn_rck,
        }
    }
}

#[intern] fn @alloc_tensor_2_f32 (manager: PassManager, size : [i64 * 2]) = @alloc_tensor_layout_2[f32](manager, size, AllocLayout::KCR);
#[intern] fn @alloc_initializer_2_f32 (manager: PassManager, size : [i64 * 2]) = @alloc_tensor_layout_2[f32](manager, size, AllocLayout::RCK);

fn @alloc_tensor_layout_3[T] (manager: PassManager, size : [i64 * 3], layout : AllocLayout) -> Tensor3[T] {
    let mut size_buffer = 1 as i64;
    for i in unroll (0, 3) {
        size_buffer *= size(i);
    }

    let buffer = manager.alloc(sizeof[T]() * size_buffer);

    let access_fn_rck = |n : [i64 * 3]| {
        let mut address = n(3 - 1);
        for i in unroll(0, 3 - 1) {
            address = address * size(3 - 2 - i) + n(3 - 2 - i);
        }
        &mut bitcast[&mut[T]](buffer.data)(address)
    };

    let access_fn_kcr = |n : [i64 * 3]| {
        let mut address = n(0);
        for i in unroll(1, 3) {
            address = address * size(i) + n(i);
        }
        &mut bitcast[&mut[T]](buffer.data)(address)
    };

    Tensor3[T] {
        buffer = buffer,
        size = size,
        access_fn = match layout { 
            AllocLayout::KCR => access_fn_kcr,
            AllocLayout::RCK => access_fn_rck,
        }
    }
}

#[intern] fn @alloc_tensor_3_f32 (manager: PassManager, size : [i64 * 3]) = @alloc_tensor_layout_3[f32](manager, size, AllocLayout::KCR);
#[intern] fn @alloc_initializer_3_f32 (manager: PassManager, size : [i64 * 3]) = @alloc_tensor_layout_3[f32](manager, size, AllocLayout::RCK);

fn @alloc_tensor_layout_4[T] (manager: PassManager, size : [i64 * 4], layout : AllocLayout) -> Tensor4[T] {
    let mut size_buffer = 1 as i64;
    for i in unroll (0, 4) {
        size_buffer *= size(i);
    }

    let buffer = manager.alloc(sizeof[T]() * size_buffer);

    let access_fn_rck = |n : [i64 * 4]| {
        let mut address = n(4 - 1);
        for i in unroll(0, 4 - 1) {
            address = address * size(4 - 2 - i) + n(4 - 2 - i);
        }
        &mut bitcast[&mut[T]](buffer.data)(address)
    };

    let access_fn_kcr = |n : [i64 * 4]| {
        let mut address = n(0);
        for i in unroll(1, 4) {
            address = address * size(i) + n(i);
        }
        &mut bitcast[&mut[T]](buffer.data)(address)
    };

    Tensor4[T] {
        buffer = buffer,
        size = size,
        access_fn = match layout { 
            AllocLayout::KCR => access_fn_kcr,
            AllocLayout::RCK => access_fn_rck,
        }
    }
}

#[intern] fn @alloc_tensor_4_f32 (manager: PassManager, size : [i64 * 4]) = @alloc_tensor_layout_4[f32](manager, size, AllocLayout::KCR);
#[intern] fn @alloc_initializer_4_f32 (manager: PassManager, size : [i64 * 4]) = @alloc_tensor_layout_4[f32](manager, size, AllocLayout::RCK);

/* Multidimensional loops */

enum LoopMode {
    Sequential,
    Vectorized,
    Parallel
}

struct LoopInstance {
    manager : PassManager,
    num_dims : i32,
    index : &mut[i64],
    min : &[i64],
    max : &[i64],
    mode : &[LoopMode]
}

#[import (cc = "plugin", name = "build_static_array")] fn static_array [T] (_element: T, _size: i64) -> &[T];
#[import (cc = "plugin", name = "static_array_set_element", depends = static_array[T])] fn static_array_set_element [T] (_element: &T, _value: T) -> ();

fn @make_loopinstance(manager : PassManager, num_dims : i32, n : &[i64]) -> LoopInstance {
    let index_buffer = manager.alloc(sizeof[i64]() * num_dims as i64);
    
    LoopInstance {
        manager = manager,
        num_dims = num_dims,
        index = bitcast[&mut[i64]](index_buffer.data),
        min = static_array(0 as i64, num_dims as i64),
        max = n,
        mode = static_array(LoopMode::Sequential, num_dims as i64)
    }
}

fn @multi_loop (body: fn(LoopInstance) -> ()) = @|manager: PassManager, num_dims: i32, n: &[i64]| {
    let instance = make_loopinstance(manager, num_dims, n);

    multi_loop_instanced(body, 0, num_dims, instance);
};

fn @multi_loop_vector (body: fn(LoopInstance) -> ()) = @|manager: PassManager, num_dims: i32, n: &[i64]| {
    let instance = make_loopinstance(manager, num_dims, n);

    static_array_set_element(&instance.mode(0), LoopMode::Vectorized);

    multi_loop_instanced(body, 0, num_dims, instance);
};

fn @(?current_dim & ?num_dims) multi_loop_instanced (body: fn(LoopInstance) -> (), current_dim: i32, num_dims: i32, instance : LoopInstance) -> () {
    //pe_info("Current dim", current_dim);
    if current_dim < num_dims {
        let mode = instance.mode(current_dim);

        match mode {
            LoopMode::Sequential => {
                for i in range(instance.min(current_dim) as i32, instance.max(current_dim) as i32) {
                    instance.index(current_dim) = i as i64;
                    multi_loop_instanced(body, current_dim + 1, num_dims, instance);
                }
            },
            LoopMode::Vectorized => {
                //let mut vector_buffers : [Buffer * vector_width];
                let mut vector_buffers : [Buffer * 8];

                for i in unroll(0, vector_width) {
                    let index_buffer_alloc = instance.manager.alloc(sizeof[i64]() * instance.num_dims as i64);
                    //let index_buffer_alloc = alloc_cpu(sizeof[i64]() * instance.num_dims as i64);

                    let index_buffer = bitcast[&mut[i64]](index_buffer_alloc.data);
                    for j in unroll(0, current_dim) {
                        index_buffer(j) = instance.index(j);
                    }

                    vector_buffers(i) = index_buffer_alloc;
                }

                for i in vector_range(0, (instance.max(current_dim) - instance.min(current_dim)) as i32) {
                    let index_buffer_alloc = vector_buffers(i % vector_width);
                    let index_buffer = bitcast[&mut[i64]](index_buffer_alloc.data);

                    let current_instance = LoopInstance {
                        manager = instance.manager,
                        num_dims = instance.num_dims,

                        index = index_buffer,
                        max = instance.max,
                        min = instance.min,
                        mode = instance.mode
                    };

                    current_instance.index(current_dim) = i as i64 + instance.min(current_dim);

                    //multi_loop_instanced(body, current_dim + 1, num_dims, current_instance);
                }
            },

            _ =>
            @body(instance)
        }
    } else {
        @body(instance);
    }
}

/* Main entry point, define the allocation and release of memory */

#[intern]
fn sequential (body: fn(PassManager) -> ()) = @|| {
    let manager = PassManager {
        alloc = get_buffer_managed,
        alloc_dynamic = get_buffer_dynamic,
        release = @|buffer| { release_buffer_managed(buffer); },
    };
    @body(manager);
};
