#[export] static mut time_conv : i64;

#[intern]
fn @matrix_convolution_padded (manager: PassManager, input: Tensor[f32], weight: Tensor[f32], bias: Tensor[f32], output_size: &[i64], _kernel_shape: &[i64], _stride: &[i64], _padding: &[i64]) -> Tensor[f32] {
    let start = get_micro_time();

    /* input shape:  [ X x Y x in_chan ]
       weight shape: [ Xk x Yk x in_chan x out_chan ]
       bias shape:   [ out_chan ]
       output shape: [ Xo x Yo x out_chan ]
    */

    fn @output_size_fn(n : i32) -> i64 { output_size(n) }
    let output = @alloc_tensor_layout[f32](manager, 3, output_size_fn, AllocLayout::RCK);

    let num_in_chan = input.size_fn(2) as i32;
    let num_out_chan = bias.size_fn(0) as i32;

    let kern_radius = (weight.size_fn(0) / 2) as i32;

    for x in range(kern_radius, input.size_fn(0) as i32 - kern_radius) {
        for y in range(kern_radius, input.size_fn(1) as i32 - kern_radius) {
            for out_chan in range(0, num_out_chan) {
                let mut sum = 0 as f32;

                for in_chan in range(0, num_in_chan) {
                    for xk in range(- kern_radius, kern_radius + 1) {
                        for yk in range(- kern_radius, kern_radius + 1) {
                            let data = *input.access_fn([(x + xk) as i64, (y + yk) as i64, in_chan as i64]);
                            let weight = *weight.access_fn([(xk + kern_radius) as i64, (yk + kern_radius) as i64, in_chan as i64, out_chan as i64]);

                            sum += data * weight;
                        }
                    }
                }

                sum += *bias.access_fn([out_chan as i64]);

                //sum = out_chan as f32 + x as f32 * 0.01 + y as f32 * 0.0001;

                *output.access_fn([(x - kern_radius) as i64, (y - kern_radius) as i64, out_chan as i64]) = sum;
            }
        }
    }

    time_conv += get_micro_time() - start;

    print_tensor("Conv", output);

    output
}
