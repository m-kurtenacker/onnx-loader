#[intern]
fn @matrix_gemm_setup(output_size: (i32, &[i64])) -> Tensor[f32] {
    fn @output_size_fn (n : i32) -> i64 { output_size.1(n) };

    let output = @alloc_tensor[f32] (output_size.0, output_size_fn);

    output
}

#[intern]
fn @matrix_gemm_impl (A: Tensor[f32], B: Tensor[f32], C: Tensor[f32], output_size: (i32, &[i64]), output: Tensor[f32]) -> Tensor[f32] {
    for out_target in multi_loop(output_size.0, output_size.1) {
        let mut sum = *C.access_fn(out_target.index);

        for k in range(0, A.size_fn(1) as i32) { //TODO: not sure if dim 1 or dim 2.
            let a = *A.access_fn([out_target.index(0), k as i64]);
            let b = *B.access_fn([k as i64, out_target.index(1)]);

            sum += a * b;
        }

        *output.access_fn(out_target.index) = sum;
    }

    output
}


fn @matrix_gemm_f32 (A: Tensor[f32], B: Tensor[f32], C: Tensor[f32], output_size: (i32, &[i64])) -> Tensor[f32] {
    let setup = matrix_gemm_setup(output_size);

    let output = matrix_gemm_impl(A, B, C, output_size, setup);

    print_tensor("Gemm", output);

    output
}


#[intern]
fn @matrix_gemm_accelerated_setup (nvvm: Accelerator, output_size: (i32, &[i64])) {
    fn @output_size_fn (n : i32) -> i64 { output_size.1(n) };

    let output_buffer = nvvm.alloc(sizeof[f16]() * output_size.1(0) * output_size.1(1));
    let output = @build_tensor[f16] (output_buffer, output_size.0, output_size_fn);

    output
}

#[intern]
fn @matrix_gemm_accelerated_impl (nvvm: Accelerator, A: Tensor[f16], B: Tensor[f16], C: Tensor[f16], output_size: (i32, &[i64]), setup: Tensor[f16]) -> Tensor[f16] {
    let output = setup;

    //TODO: This has only been tested for quadratic matricies, it is likely that the dimensions are messed up.
    let N = output_size.1(0) as i32;
    let M = output_size.1(1) as i32;
    let K = output_size.1(0) as i32; //TODO: argument?

    let a_matrix = Matrix { data = bitcast[&mut[f16]](A.buffer.data), x_dim = K, y_dim = M, addr_mode = A.addr_mode, stride = match A.addr_mode { AddrMode::RowMayor => K, AddrMode::ColMayor => M } };
    let b_matrix = Matrix { data = bitcast[&mut[f16]](B.buffer.data), x_dim = N, y_dim = K, addr_mode = B.addr_mode, stride = match B.addr_mode { AddrMode::RowMayor => N, AddrMode::ColMayor => K } };
    let c_matrix = Matrix { data = bitcast[&mut[f16]](C.buffer.data), x_dim = N, y_dim = M, addr_mode = C.addr_mode, stride = match C.addr_mode { AddrMode::RowMayor => N, AddrMode::ColMayor => M } };
    let d_matrix = Matrix { data = bitcast[&mut[f16]](output.buffer.data), x_dim = N, y_dim = M, addr_mode = C.addr_mode, stride = match C.addr_mode { AddrMode::RowMayor => N, AddrMode::ColMayor => M } };

    matrix_multiply_nvvm(nvvm, a_matrix, b_matrix, c_matrix, d_matrix);

    output
}
