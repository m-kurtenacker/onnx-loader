#[intern]
fn @matrix_gemm_setup(output_size: (i32, &[i64])) -> Tensor[f32] {
    fn @output_size_fn (n : i32) -> i64 { output_size.1(n) };

    let output = @alloc_tensor[f32] (output_size.0, output_size_fn);

    output
}

#[intern]
fn @matrix_gemm_impl (A: Tensor[f32], B: Tensor[f32], C: Tensor[f32], output_size: (i32, &[i64]), output: Tensor[f32]) -> Tensor[f32] {
    for out_target in multi_loop(output_size.0, output_size.1) {
        let mut sum = *C.access_fn(out_target.index);

        for k in range(0, A.size_fn(1) as i32) { //TODO: not sure if dim 1 or dim 2.
            let a = *A.access_fn([out_target.index(0), k as i64]);
            let b = *B.access_fn([k as i64, out_target.index(1)]);

            sum += a * b;
        }

        *output.access_fn(out_target.index) = sum;
    }

    output
}


fn @matrix_gemm_f32 (A: Tensor[f32], B: Tensor[f32], C: Tensor[f32], output_size: (i32, &[i64])) -> Tensor[f32] {
    let setup = matrix_gemm_setup(output_size);

    let output = matrix_gemm_impl(A, B, C, output_size, setup);

    print_tensor("Gemm", output);

    output
}


#[intern]
fn @matrix_gemm_accelerated_setup (output_size: (i32, &[i64])) {
    let nvvm = nvvm_accelerator(0);

    fn @output_size_fn (n : i32) -> i64 { output_size.1(n) };

    let output = @alloc_tensor[f16] (output_size.0, output_size_fn);

    let N = output_size.1(0);
    let M = output_size.1(1);
    let K = output_size.1(0); //TODO: argument?

    let acontainer = nvvm.alloc(sizeof[f16]() * M * K);
    let bcontainer = nvvm.alloc(sizeof[f16]() * K * N);
    let ccontainer = nvvm.alloc(sizeof[f16]() * M * N);
    let dcontainer = nvvm.alloc(sizeof[f16]() * M * N);

    (nvvm, acontainer, bcontainer, ccontainer, dcontainer, output)
}

#[intern]
fn @matrix_gemm_accelerated_impl (A: Tensor[f16], B: Tensor[f16], C: Tensor[f16], output_size: (i32, &[i64]), setup: (Accelerator, Buffer, Buffer, Buffer, Buffer, Tensor[f16])) -> Tensor[f16] {
    let (nvvm, acontainer, bcontainer, ccontainer, dcontainer, output) = setup;

    copy(A.buffer, acontainer);
    copy(B.buffer, bcontainer);
    copy(C.buffer, ccontainer);

    //TODO: This has only been tested for quadratic matricies, it is likely that the dimensions are messed up.
    let N = output_size.1(0) as i32;
    let M = output_size.1(1) as i32;
    let K = output_size.1(0) as i32; //TODO: argument?

    let a_matrix = Matrix { data = bitcast[&mut[f16]](acontainer.data), x_dim = K, y_dim = M, addr_mode = A.addr_mode, stride = match A.addr_mode { AddrMode::RowMayor => K, AddrMode::ColMayor => M } };
    let b_matrix = Matrix { data = bitcast[&mut[f16]](bcontainer.data), x_dim = N, y_dim = K, addr_mode = B.addr_mode, stride = match B.addr_mode { AddrMode::RowMayor => N, AddrMode::ColMayor => K } };
    let c_matrix = Matrix { data = bitcast[&mut[f16]](ccontainer.data), x_dim = N, y_dim = M, addr_mode = C.addr_mode, stride = match C.addr_mode { AddrMode::RowMayor => N, AddrMode::ColMayor => M } };
    let d_matrix = Matrix { data = bitcast[&mut[f16]](dcontainer.data), x_dim = N, y_dim = M, addr_mode = C.addr_mode, stride = match C.addr_mode { AddrMode::RowMayor => N, AddrMode::ColMayor => M } };

    matrix_multiply_nvvm(nvvm, a_matrix, b_matrix, c_matrix, d_matrix);

    copy(dcontainer, output.buffer);

    output
}
